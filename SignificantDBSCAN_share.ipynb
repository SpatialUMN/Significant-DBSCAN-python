{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignificantDBSCAN_share.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgK_7VH_nfKm"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KveV9trcJS-i"
      },
      "source": [
        "Function definitions first; demo at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-j1L_bstnlI"
      },
      "source": [
        "def estimate_density(X, h, complete_random = 2):\n",
        "    \"\"\"Estimate frequency distribution along each dimension.\n",
        "    Assuming dimensions are independent. Preprocessing (e.g., dimension reduction) \n",
        "    are recommended for data containing correlated dimensions.\n",
        "\n",
        "    Paratemers\n",
        "    -----------\n",
        "    X: Input data. Feature values should be in range [0,1]\n",
        "    h: number of bins for frequency calculation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dist_d: Numpy array. Distribution (freq.) of data points along each dimension \n",
        "    (distribution along each dimension is represented as a vector with h bins).\n",
        "    \"\"\"\n",
        "\n",
        "    n, d = X.shape#number of points and dimensions\n",
        "    dist_d = np.zeros([d,h])\n",
        "\n",
        "    if complete_random == 1:\n",
        "        dist_d = dist_d + 1/h\n",
        "        return dist_d\n",
        "\n",
        "    bin_width = 1/h\n",
        "    X_bin = np.floor(X / bin_width).astype(int)\n",
        "    #make max value to h-1 bin (or use a more efficient fix by adding a very small value to max for each dimension)\n",
        "    X_bin[X_bin == h] = h - 1\n",
        "    \n",
        "    for i in range(d):\n",
        "        unique, counts = np.unique(X_bin[:,i], return_counts=True)\n",
        "        for j in range(unique.shape[0]):\n",
        "            bin_id = unique[j]\n",
        "            dist_d[i, bin_id] = counts[j] / n\n",
        "    \n",
        "    if complete_random == 2:\n",
        "        dist_d_random = 1/h\n",
        "        data_weight = 0.75\n",
        "        random_weight = 1 - data_weight\n",
        "        dist_d = data_weight * dist_d + random_weight * dist_d_random\n",
        "\n",
        "    return dist_d\n",
        "\n",
        "def generate_H0_data(n, dist_d):\n",
        "    \"\"\"Estimate test statistic for spurious cluster removal.\n",
        "\n",
        "    Paratemers\n",
        "    -----------\n",
        "    n: Data size\n",
        "    dist_d: Numpy array. Distribution (freq.) of data points along each dimension \n",
        "    (distribution along each dimension is represented as a vector with h bins). Example: [[0.4, 0.6], [0.1, 0.9]]\n",
        "    In addition, the range of valid values (not freq.) along each dimension is assumed to be in [0,1]. \n",
        "    \"\"\"\n",
        "    d = dist_d.shape[0]#number of dimensions\n",
        "    h = dist_d.shape[1]#number of bins to calculate the frequency distribution along each dimenison\n",
        "    \n",
        "    h0_data = np.zeros([n,d])\n",
        "    #get the min value for each bin;again, range of valid values assumed to be in [0,1]\n",
        "    bin_base = np.arange(h)/h\n",
        "    for i in range(d):\n",
        "        #get bin selections randomly for each point\n",
        "        bin_random = np.random.choice(h, n, p=dist_d[i,:])\n",
        "        # h0_data[:,i] = np.reshape(bin_base[bin_random], [-1,1]) + np.random.rand(n,1)/h\n",
        "        h0_data[:,i] = bin_base[bin_random] + np.random.rand(n)/h\n",
        "\n",
        "    return h0_data\n",
        "\n",
        "def get_max_cluster_size(cluster_labels):\n",
        "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
        "    count_array = np.vstack([unique, counts]).T#get unique labels and counts\n",
        "    count_array = count_array[np.argsort(count_array[:, 1])]#sort by count (2nd column)\n",
        "    count_array = count_array[::-1,:]#reverse the sort order --> descending\n",
        "    #get the max size\n",
        "    max_size = 0\n",
        "    if count_array[0,0] == -1:#label \"-1\" means noise\n",
        "        if count_array.shape[0] > 1:\n",
        "            max_size = count_array[1,1]\n",
        "    else:\n",
        "        max_size = count_array[0,1]\n",
        "    return max_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XHfRuJato_3"
      },
      "source": [
        "def monte_carlo_estimation(n, dist_d, m, sig_level, eps, minpts, h=10, best_obs_cluster = np.inf, print_freq = 10, print_option = 1):\n",
        "    \"\"\"Estimate test statistic for spurious cluster removal.\n",
        "\n",
        "    Paratemers\n",
        "    -----------\n",
        "    X: Input data. Feature values should be in range [0,1]\n",
        "    h: number of bins for frequency calculation\n",
        "    dist_d: Numpy array. Distribution (freq.) of data points along each dimension (distribution along each dimension is represented as a vector with h bins). Example: [[0.4, 0.6], [0.1, 0.9]]\n",
        "    m: Number of simulation trials, e.g., 19, 99, 999\n",
        "    sig_level: Significance level, e.g., 0.05, 0.01\n",
        "    eps: eps for DBSCAN (must be the same for observed data)\n",
        "    minpts: minpts for DBSCAN (must be the same for observed data)\n",
        "    best_obs_cluster: size of the largest cluster detected from observed data; if a value is provided, will be used to enable early-termination\n",
        "    print_freq: how often does the function print about the trial id being executed (help user to estimate time left)\n",
        "    \"\"\"\n",
        "\n",
        "    if print_option == 1:\n",
        "        print('Monte Carlo estimation started (may take some time to finish)...')\n",
        "        print('Total trial number: ', m)\n",
        "\n",
        "    monte_carlo_table = np.zeros(m)\n",
        "    early_term_cnt = 0\n",
        "    for i in range(m):\n",
        "        h0_data = generate_H0_data(n, dist_d)\n",
        "        clusterer = DBSCAN(eps, min_samples=minpts).fit(h0_data)\n",
        "        monte_carlo_table[i] = get_max_cluster_size(clusterer.labels_)\n",
        "        if monte_carlo_table[i] >= best_obs_cluster:\n",
        "            early_term_cnt += 1\n",
        "            if early_term_cnt >= np.ceil(m * sig_level):\n",
        "                if print_option == 1:\n",
        "                    print(\"Terminated early: no significant clusters...\")\n",
        "                return np.inf\n",
        "        \n",
        "        if i % print_freq == 0:\n",
        "            if print_option == 1:\n",
        "                # print('Trial ', i, ', ', m-i-1, ' trials to complete...')\n",
        "                print(i, 'trials completed...')\n",
        "    \n",
        "    monte_carlo_table = np.sort(monte_carlo_table)#sort\n",
        "    monte_carlo_table = monte_carlo_table[::-1]#reverse order --> descending\n",
        "    idx = np.ceil(m * sig_level).astype(int)\n",
        "    threshold = monte_carlo_table[idx]\n",
        "    \n",
        "    # print(monte_carlo_table)\n",
        "    \n",
        "    return threshold\n",
        "\n",
        "def remove_spurious_cluster(cluster_labels, threshold):\n",
        "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
        "    count_array = np.vstack([unique, counts]).T#get unique labels and counts\n",
        "    \n",
        "    spurious_cluster_ids = count_array[np.where(count_array[:,1]<=threshold), 0]#cluster ids\n",
        "    spurious_points = np.isin(cluster_labels, spurious_cluster_ids)#true or false (this list has equal size to cluster_labels, so true/false can be used to select rows)\n",
        "    cluster_labels[spurious_points] = -1\n",
        "    \n",
        "    return cluster_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYkxB34QrUSc"
      },
      "source": [
        "def get_eps(X, eps_list, minpts_list, i_current, j_current, increase_thrd):\n",
        "    '''\n",
        "    Heuristically find which eps to use (for a fixed density), starting from smallest.\n",
        "    Move to a larger one if the mean size of clusters increases by at least increase_thrd (a proportion of previous mean).\n",
        "    In general, lower density clusters tend to need a larger eps.\n",
        "    Here the maximum number of clusters to use for mean-size calculation can be changed by users; defaulted to top 5 clusters.\n",
        "    '''\n",
        "    \n",
        "    # #for eps, use a relaxed increase_thrd\n",
        "    # increase_thrd = increase_thrd/2\n",
        "\n",
        "    max_j = eps_list.shape[0]\n",
        "\n",
        "    cluster_mean_size_prev = 0\n",
        "    labels_prev = None\n",
        "\n",
        "    for j in range(j_current, max_j):\n",
        "        eps = eps_list[j]\n",
        "        minpts = minpts_list[i_current,j]\n",
        "\n",
        "        clusterer = DBSCAN(eps=eps, min_samples=minpts).fit(X)\n",
        "        unique, counts = np.unique(clusterer.labels_, return_counts=True)\n",
        "        count_array = np.vstack([unique, counts]).T#get unique labels and counts\n",
        "        #remove -1 noise points before mean calculation\n",
        "        cluster_ids = count_array[:,0] >= 0\n",
        "        count_array = count_array[cluster_ids,:]\n",
        "        #add sort\n",
        "        count_array = count_array[np.argsort(count_array[:, 1])]#sort by count (2nd column)\n",
        "        count_array = count_array[::-1,:]\n",
        "\n",
        "        if count_array.shape[0] == 0:\n",
        "            #no clusters\n",
        "            cluster_mean_size = 0\n",
        "        else:\n",
        "            max_to_check = 10\n",
        "            check_size = min(count_array.shape[0], max_to_check)\n",
        "            cluster_mean_size = np.mean(count_array[0:check_size,1])\n",
        "            # cluster_mean_size = np.mean(count_array[:,1])\n",
        "\n",
        "        if cluster_mean_size_prev == 0:\n",
        "            cluster_mean_size_prev = cluster_mean_size\n",
        "            labels_prev = np.array(clusterer.labels_)#might not need the np.array()\n",
        "        elif (cluster_mean_size - cluster_mean_size_prev) / cluster_mean_size_prev > increase_thrd:\n",
        "            cluster_mean_size_prev = cluster_mean_size\n",
        "            labels_prev = np.array(clusterer.labels_)#might not need the np.array()\n",
        "        else:\n",
        "            return j-1, labels_prev, cluster_mean_size_prev\n",
        "    \n",
        "        if j == (max_j-1):\n",
        "            return j, np.array(clusterer.labels_), cluster_mean_size\n",
        "\n",
        "\n",
        "def get_minpts(X, eps_list, minpts_list, i_current, j_current, increase_thrd, labels_prev, cluster_mean_size_prev):\n",
        "    '''\n",
        "    Heuristically find which density (minpts) to use for a fixed eps, starting from the largest.\n",
        "    (note: part of this function may be merged with get_eps() later)\n",
        "    '''\n",
        "    \n",
        "    max_i = minpts_list.shape[0]\n",
        "\n",
        "    # prev results inherited from get_eps()'s outputs\n",
        "    # cluster_mean_size_prev = 0\n",
        "    # labels_prev = None\n",
        "    for i in range(i_current, max_i):\n",
        "        eps = eps_list[j_current]\n",
        "        minpts = minpts_list[i,j_current]\n",
        "\n",
        "        clusterer = DBSCAN(eps=eps, min_samples=minpts).fit(X)\n",
        "        unique, counts = np.unique(clusterer.labels_, return_counts=True)\n",
        "        count_array = np.vstack([unique, counts]).T#get unique labels and counts\n",
        "        #remove -1 noise points before mean calculation\n",
        "        cluster_ids = count_array[:,0] >= 0\n",
        "        count_array = count_array[cluster_ids,:]\n",
        "        #add sort\n",
        "        count_array = count_array[np.argsort(count_array[:, 1])]#sort by count (2nd column)\n",
        "        count_array = count_array[::-1,:]\n",
        "\n",
        "        if count_array.shape[0] == 0:\n",
        "            #no clusters\n",
        "            cluster_mean_size = 0\n",
        "        else:\n",
        "            max_to_check = 10\n",
        "            check_size = min(count_array.shape[0], max_to_check)\n",
        "            cluster_mean_size = np.mean(count_array[0:check_size,1])\n",
        "            # cluster_mean_size = np.mean(count_array[:,1])\n",
        "\n",
        "        if cluster_mean_size_prev == 0:\n",
        "            cluster_mean_size_prev = cluster_mean_size\n",
        "            labels_prev = np.array(clusterer.labels_)#might not need the np.array()\n",
        "        elif (cluster_mean_size - cluster_mean_size_prev) / cluster_mean_size_prev > increase_thrd:\n",
        "            cluster_mean_size_prev = cluster_mean_size\n",
        "            labels_prev = np.array(clusterer.labels_)#might not need the np.array()\n",
        "        else:\n",
        "            return i-1, labels_prev, cluster_mean_size_prev\n",
        "        \n",
        "        if i == (max_i-1):\n",
        "            return i, np.array(clusterer.labels_), cluster_mean_size\n",
        "\n",
        "    \n",
        "def SigDBSCAN(X, eps_range, density_range, m=100, sig_level=0.01, h=10, \n",
        "              num_eps = 5, num_density = 5, sample_portion = 0.5, \n",
        "              increase_thrd = 0.1, complete_random = 2, print_option = 1):\n",
        "    \"\"\"\n",
        "    This version is for arbitrary dimensions, and has not included the grid-based accelerations. Early-termination speed-up is included which mainly works for non-clustered data.\n",
        "    Consider using a sampled version of the original dataset if it is too large (results may vary).\n",
        "\n",
        "    For high-dimensional data, it is recommended to use dimension reduction or deep embedding methods to reduce the feature space.\n",
        "\n",
        "    Paratemers\n",
        "    -----------\n",
        "    X: Input data. Feature values should be in range [0,1]\n",
        "\n",
        "    eps_range: Minimum and maximum eps to consider (X feature value range is in [0,1], so this number needs to be in [0,1])\n",
        "    \n",
        "    density_range: SigDBSCAN enumerates through a list of densities estimated from X. \n",
        "        This range means the max and min relative density to consider. For example, denote D as the distribution of densities around all points for a given eps, \n",
        "        a [min, max] = [0.5, 0.9] means CDF(D, min_density) = 0.5, and the max is CDF(D, max_density) = 0.9. CDF is cumulative probability function.\n",
        "    \n",
        "    m: Number of simulation trials, e.g., 19, 99, 999\n",
        "    \n",
        "    sig_level: Significance level, e.g., 0.05, 0.01\n",
        "    \n",
        "    h: number of bins for frequency calculation\n",
        "    \n",
        "    num_eps: number of eps to consider in the eps range\n",
        "    \n",
        "    num_density: number of densities to consider in the density range\n",
        "    \n",
        "    sample_portion: proprtion of data points to use to estimate density distribution (default to 1).\n",
        "        This is used for both single dimension (for H0 data generation) and all dimensions (used to estimate min and max density for density range)\n",
        "    \n",
        "    increase_thrd: When deciding whether to move to the a larger eps or a lower density\n",
        "        (choosing (eps, minpts) in a heuristic manner; happens before each significance testing).\n",
        "        Illustration all candidates for default num_eps = 5 and num_density = 5: (the current search only moves to larger eps or lower density; \n",
        "        it starts from the next density with the same eps after each significance testing; final result is an aggregation of significant clusters)\n",
        "                    eps1,               eps2,   ...,    eps5\n",
        "        density1    (eps1, minpts1),    ...,\n",
        "        ...         ...,           ,    ..., \n",
        "        density5    (eps5, minpts5),    ...,\n",
        "    \n",
        "    complete_random: determines the null distribution for each dimension. \n",
        "        If 1, then H0 data points are completely random in each dimension (e.g., h bins receive equal probability).\n",
        "        If 0, then probability distribution is estimated using input data X (this might be better for higher dimensions, where data are sparse).\n",
        "        If 2, then probability distribution is a weighted average of the above two scenarios (weights are defaulted to [0.5, 0.5] and can be changed in estimate_density() function)\n",
        "        Users can define their own null distribution (using estimate_density() function).\n",
        "        The null distribution affects the significance of clusters.\n",
        "\n",
        "    print_option: print key steps and progress if 1; otherwise, no or minimal print\n",
        "    \"\"\"\n",
        "    #data attributes\n",
        "    n,d = X.shape\n",
        "    # vis(X)\n",
        "\n",
        "    y = np.zeros([n]) - 1\n",
        "    X_id = np.arange(0, n)#data will be updated during Significant DBSCAN, X_id keeps track of indices of data points in the original data\n",
        "    cluster_id_base = 0 #increase after each round of cluster detetcion (for a density level), making sure no duplicate cluster ids when adding labels of new clusters\n",
        "\n",
        "    eps_list = np.linspace(eps_range[0], eps_range[1], num=num_eps)\n",
        "    eps_med = np.median(eps_list)\n",
        "\n",
        "    #estimate distribution of density (using the median eps to get densities around sample points in data),\n",
        "    #and generate eps_list and minpts_list as candidates for heuristic search\n",
        "    density_list_full = np.zeros([n])#use size n first (if n is too large, better first sample points)\n",
        "    cnt = 0 #counter for number of samples actually used (after rand)\n",
        "    for i in range(n):\n",
        "        if np.random.rand() >= sample_portion:\n",
        "            continue\n",
        "        \n",
        "        center = X[i,:]\n",
        "        distance = norm(X - center, 2, axis = -1)\n",
        "        density_list_full[cnt] = np.sum(distance <= eps_med) / (math.pi * (eps_med**2))\n",
        "\n",
        "        cnt += 1\n",
        "    \n",
        "    density_list_full = np.sort(density_list_full[0:cnt])\n",
        "    #mode 1: get equal_size bins from min to max density value\n",
        "    density_min_value = density_list_full[np.floor(density_range[0]*cnt).astype(int)]\n",
        "    density_max_value = density_list_full[np.floor(density_range[1]*cnt).astype(int)]\n",
        "    density_list = np.linspace(density_min_value, density_max_value, num=num_density)\n",
        "    #mode 2: get equal_size bins from min to max density proportion (defined in density_range), and then get the corresponding density values\n",
        "    # density_range_steps = np.linspace(density_range[0], density_range[1], num=num_density)\n",
        "    # density_list = np.zeros([num_density])\n",
        "    # for i in range(num_density):\n",
        "    #     density_list[i] = density_list_full[np.floor(density_range_steps[i]*cnt).astype(int)]\n",
        "\n",
        "    #descending order\n",
        "    density_list = density_list[::-1]\n",
        "\n",
        "    minpts_list = np.zeros([num_density, num_eps])\n",
        "    for i in range(num_density):\n",
        "        for j in range(num_eps):\n",
        "            minpts_list[i,j] = np.ceil(density_list[i] * math.pi * eps_list[j]**2)\n",
        "\n",
        "    if print_option == 1:\n",
        "        print('eps_list: ', eps_list)\n",
        "        print('minpts_list:\\n', minpts_list)\n",
        "\n",
        "    sig = True\n",
        "    best_i = 0 #id for minpts_list's dimension 0 (rows)\n",
        "    best_j = 0 #id for eps_list, starting from smallest (does not decrease for lower density)\n",
        "    best_y = None\n",
        "    best_cluster_mean_size_current = 0\n",
        "    X_id_current = np.copy(X_id)#ids of data points current X (updated in while loop) in the original X\n",
        "    while sig:\n",
        "        #data size is updated after removal of points in significant clusters.\n",
        "        #need to update as this affects H0 data generation in Monte Carlo estimation\n",
        "        n = X.shape[0]\n",
        "        \n",
        "        #select eps and minpts: heuristic described in paper\n",
        "        best_j, best_y, best_cluster_mean_size_current = get_eps(X, eps_list, minpts_list, best_i, best_j, increase_thrd)\n",
        "        if best_i < num_density - 1:\n",
        "            best_i, best_y, _ = get_minpts(X, eps_list, minpts_list, best_i+1, best_j, increase_thrd, best_y, best_cluster_mean_size_current)\n",
        "        eps = eps_list[best_j]\n",
        "        minpts = minpts_list[best_i, best_j]\n",
        "        if print_option == 1:\n",
        "            print('\\n-----------------------------new round-----------------------------')\n",
        "            print('Data size (changes after significant cluster removal): ', n)\n",
        "            print('Selected (eps, minpts) for the current round: (%f, %d)' % (eps, minpts))\n",
        "\n",
        "        #total number of detected clusters\n",
        "        num_cluster = max(0, np.max(best_y)+1)\n",
        "        best_obs_cluster = get_max_cluster_size(best_y)\n",
        "        if best_obs_cluster == 0:\n",
        "            print('No clusters detected at the current density level.')\n",
        "            break\n",
        "        \n",
        "        #significance testing\n",
        "        #be careful here with the estimation method to use (e.g., completely random or not)\n",
        "        dist_d = estimate_density(X, h, complete_random)\n",
        "        thrd = monte_carlo_estimation(n, dist_d, m, sig_level, eps, minpts, \n",
        "                                      best_obs_cluster = best_obs_cluster, print_option = print_option)\n",
        "        best_y = remove_spurious_cluster(best_y, thrd)\n",
        "\n",
        "\n",
        "        if np.max(best_y) < 0:\n",
        "            if print_option == 1:\n",
        "                print('Total number of clusters detected: ', num_cluster)\n",
        "                print('No significant cluster found at the current density level. Continue with next...')\n",
        "                # print('No significant cluster found at the current density level. Terminating...')\n",
        "            # use the following to skip the search with lower densities if no significant cluster is found at the current level\n",
        "            #trade-off: may miss clusters if used, but will take away effect of potential multi-testing (when a large number of densities are used)\n",
        "            # sig = False\n",
        "            # break\n",
        "        else:\n",
        "            if print_option == 1:\n",
        "                print('Testing completed for current density level: ')\n",
        "                print('Total number of clusters detected: ', num_cluster)\n",
        "                unique, counts = np.unique(best_y, return_counts=True)\n",
        "                print('Total number of significant clusters: ', np.sum(unique>=0))\n",
        "                if np.sum(unique>=0) > 0:\n",
        "                    sig_cluster_sizes = np.sort(counts[unique>=0])\n",
        "                    print('New significant cluster sizes:', sig_cluster_sizes[::-1])\n",
        "        \n",
        "            #update cluster ids in y\n",
        "            sig_cluster_list = best_y >= 0\n",
        "            sig_cluster_original_id = X_id_current[sig_cluster_list]\n",
        "            y[sig_cluster_original_id] = cluster_id_base + best_y[sig_cluster_list]\n",
        "            cluster_id_base = np.max(y) + 1\n",
        "\n",
        "            #update X: remove points in significant clusters\n",
        "            #note that estimate_distribution() automatically removes space previously taken by the removed points (i.e., reducing freq. in corresponding bins)\n",
        "            X = X[sig_cluster_list==False]\n",
        "            X_id_current = X_id_current[sig_cluster_list==False]\n",
        "            # vis(X)\n",
        "\n",
        "        if best_i == num_density - 1:\n",
        "            break\n",
        "        else:\n",
        "            best_i += 1\n",
        "\n",
        "    return y.astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIO9wvxfIJuA"
      },
      "source": [
        "def normalize_data(X, range_d = None):\n",
        "    \"\"\"\n",
        "    Ideally data should be normalized by users during preprocessing.\n",
        "    This is only used to make sure all feature values are in a [0,1] range.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X: input data with d features\n",
        "    \n",
        "    range_d: A 2xd array storing the min and max range value used to normalize each dimension.\n",
        "        Note that the min, max do not need to be the min, max values obtained from data, especially\n",
        "        if value ranges should be different across dimensions.\n",
        "        If not provided, min, max values for each dimension in the data will be used for normalization.\n",
        "    \"\"\"\n",
        "    n,d = X.shape\n",
        "\n",
        "    if range_d is None:\n",
        "        range_d = np.zeros([2,d])\n",
        "        range_d[0,:] = np.min(X, axis = 0)\n",
        "        range_d[1,:] = np.max(X, axis = 0)\n",
        "\n",
        "    X = (X - range_d[0,:]) / (range_d[1,:] - range_d[0,:])\n",
        "\n",
        "    return X\n",
        "\n",
        "def vis(X, y = None, vis_noise = False):\n",
        "    \"\"\"\n",
        "    Two (or one) dimensions only.\n",
        "    Subspaces are needed for visualizing higher dimensional data.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "\n",
        "    if y is None:\n",
        "        plt.scatter(*X.T, s=1)\n",
        "    else:\n",
        "        color_noise = (1,1,1)\n",
        "        if vis_noise:\n",
        "            color_noise = (0.75, 0.75, 0.75)\n",
        "        \n",
        "        color_palette = sns.color_palette('deep', np.max(y).astype(int)+1)\n",
        "        cluster_colors = [color_palette[y_i] if y_i >= 0\n",
        "                        else color_noise\n",
        "                        for y_i in y]\n",
        "        \n",
        "        plt.scatter(*X.T, s=1, c=cluster_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9xOMXCAlxB"
      },
      "source": [
        "#demo\n",
        "\n",
        "#load, normalize and visualize data (visualization is only for 2D data)\n",
        "file_path = 'shapedata_example_1.txt'\n",
        "X = np.loadtxt(file_path, delimiter=',')\n",
        "X = normalize_data(X)\n",
        "vis(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFxUXwCmv3AV"
      },
      "source": [
        "\"\"\"\n",
        "Run Significant DBSCAN.\n",
        "\"\"\"\n",
        "\n",
        "#define ranges for eps and density_range\n",
        "#see definitions in SigDNSCAN.py (currently eps is isotropic for different dimensions; will update later)\n",
        "eps_range = np.array([0.02, 0.05])#min & max eps\n",
        "density_range = np.array([0.25, 0.75])#min & max density (percentile in overall density distribution in data)\n",
        "\n",
        "#run SigDBSCAN (a simplified version without optimization and accelerations for low dimensional data)\n",
        "#see SignificantDBSCAN() function for detailed parameter definitions\n",
        "#not recomending increasing number_density by too much, otherwise may introduce multi-testing issue;\n",
        "#if large num_density is used, to avoid/mitigate the potential multi-testing issue, uncomment the \"break\" in SigDBSCAN() to terminate the search if no significant cluster is found at a density level\n",
        "#change print_option to 0 to hide intermediate prints\n",
        "y = SigDBSCAN(X, eps_range, density_range, m=100, sig_level=0.01, h=10, \n",
        "              num_eps = 5, num_density = 5, sample_portion = 1, \n",
        "              increase_thrd = 0.1, complete_random = 2, print_option = 1)\n",
        "\n",
        "#visualize results (only for 2D at the moment)\n",
        "vis(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}